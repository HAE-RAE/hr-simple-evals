#!/usr/bin/env python
import argparse
import os
import torch
from vllm import LLM, SamplingParams
from datasets import load_dataset
from tqdm import tqdm
from dataset_configs import get_config, DATASET_CONFIGS

# Transformers configuration ì¤‘ë³µ ë“±ë¡ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ import
from transformers import AutoConfig
from transformers.models.auto.configuration_auto import CONFIG_MAPPING

# LiteLLMì„ ìœ„í•œ API í‚¤ ì„¤ì •ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì—ì„œ
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

# Transformers configuration ì¤‘ë³µ ë“±ë¡ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

def parse_args():
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì¸ìë“¤ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    p = argparse.ArgumentParser(description="Run a vLLM model on a dataset and save its responses.")
    p.add_argument("--model", required=True, help="Hugging Face model ID or local path")
    p.add_argument("--dataset", required=True,
                   help=f"Dataset name. Supported: {list(DATASET_CONFIGS.keys())}")
    p.add_argument("--dataset_hub_id", default='HAERAE-HUB/KoSimpleEval', help="Hugging Face Hub ID for the dataset collection")
    p.add_argument("--split", default="test", help="Dataset split (default: test)")
    p.add_argument("--max_tokens", type=int, default=2048, help="Maximum tokens to generate")
    p.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature (0.0 for deterministic output)")
    p.add_argument("--top_p", type=float, default=1.0, help="Top-p / nucleus sampling (1.0 to disable)")
    p.add_argument("--output", default=None, help="Output CSV path (auto-generated if omitted)")
    return p.parse_args()

def _handle_duplicate_config_registration(model_path: str) -> None:
    """ì¤‘ë³µëœ configuration ë“±ë¡ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
        model_name = model_path.split('/')[-1].lower()
        
        # ì¼ë°˜ì ì¸ aimv2 ê´€ë ¨ ë“±ë¡ë„ í™•ì¸
        problematic_names = ['aimv2', 'aim_v2', 'aimv2_config', model_name]
        
        # CONFIG_MAPPINGì—ì„œ ì¤‘ë³µ ë“±ë¡ ì œê±°
        for name in problematic_names:
            try:
                # _extra_content ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                if hasattr(CONFIG_MAPPING, '_extra_content') and name in CONFIG_MAPPING._extra_content:
                    print(f"âš ï¸ '{name}' configurationì´ _extra_contentì— ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë“±ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.")
                    del CONFIG_MAPPING._extra_content[name]
                
                # ì§ì ‘ CONFIG_MAPPINGì— ë“±ë¡ëœ ê²½ìš°
                if name in CONFIG_MAPPING:
                    print(f"âš ï¸ '{name}' configurationì´ CONFIG_MAPPINGì— ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë“±ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.")
                    # LazyConfigMappingì—ì„œëŠ” ì§ì ‘ ì‚­ì œê°€ ì•ˆë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                    try:
                        del CONFIG_MAPPING[name]
                    except (TypeError, AttributeError):
                        # LazyConfigMappingì˜ ë‚´ë¶€ êµ¬ì¡°ì— ì§ì ‘ ì ‘ê·¼
                        if hasattr(CONFIG_MAPPING, '_mapping') and name in CONFIG_MAPPING._mapping:
                            del CONFIG_MAPPING._mapping[name]
                        if hasattr(CONFIG_MAPPING, '_extra_content') and name in CONFIG_MAPPING._extra_content:
                            del CONFIG_MAPPING._extra_content[name]
            except Exception as e:
                print(f"âš ï¸ '{name}' configuration ì œê±° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
                
        # AutoConfigì˜ _model_type_to_module_nameì—ì„œë„ ì œê±°
        try:
            from transformers.models.auto.configuration_auto import _model_type_to_module_name
            for name in problematic_names:
                if name in _model_type_to_module_name:
                    print(f"âš ï¸ '{name}' configurationì´ _model_type_to_module_nameì— ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë“±ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.")
                    del _model_type_to_module_name[name]
        except (ImportError, AttributeError):
            pass
            
        # ì¶”ê°€ì ì¸ ë“±ë¡ ìœ„ì¹˜ë“¤ë„ í™•ì¸
        try:
            from transformers.models.auto import modeling_auto
            if hasattr(modeling_auto, 'MODEL_MAPPING'):
                for name in problematic_names:
                    if name in modeling_auto.MODEL_MAPPING:
                        print(f"âš ï¸ '{name}' configurationì´ MODEL_MAPPINGì— ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë“±ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.")
                        try:
                            del modeling_auto.MODEL_MAPPING[name]
                        except Exception:
                            pass
        except (ImportError, AttributeError):
            pass
                
    except Exception as e:
        print(f"âš ï¸ Configuration ë“±ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

def main() -> None:
    """ë©”ì¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    args = parse_args()

    # 1. ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    try:
        config = get_config(args.dataset)
        prompt_maker = config['prompt_maker']
        evaluator = config['evaluator']
    except ValueError as e:
        print(f"ì˜¤ë¥˜: {e}")
        return
    
    # 2. ì¤‘ë³µ configuration ë“±ë¡ ë¬¸ì œ í•´ê²°
    _handle_duplicate_config_registration(args.model)
    
    print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘: {args.model}")
    
    # ëª¨ë¸ ë¡œë”©ì„ ì—¬ëŸ¬ ë²ˆ ì‹œë„ (configuration ë“±ë¡ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´)
    max_retries = 3
    for attempt in range(max_retries):
        try:
            llm = LLM(model=args.model, tensor_parallel_size=torch.cuda.device_count(), trust_remote_code=True)
            tokenizer = llm.get_tokenizer()
            break
        except ValueError as e:
            if "already used by a Transformers config" in str(e) and attempt < max_retries - 1:
                print(f"âš ï¸ ì‹œë„ {attempt + 1}/{max_retries}: Configuration ì¤‘ë³µ ë“±ë¡ ì˜¤ë¥˜ ë°œìƒ. ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
                _handle_duplicate_config_registration(args.model)
                continue
            else:
                print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                return
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return

    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {args.dataset_hub_id} - {args.dataset}")
    df = load_dataset(args.dataset_hub_id, args.dataset, split=args.split).to_pandas()

    print("âœï¸  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    tqdm.pandas(desc="í”„ë¡¬í”„íŠ¸ ìƒì„±")
    prompts = df.progress_apply(lambda row: prompt_maker(row, tokenizer), axis=1).tolist()

    print(f"ğŸ§  ëª¨ë¸ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤ (ì´ {len(prompts)}ê°œ)...")
    sampling_params = SamplingParams(temperature=args.temperature, top_p=args.top_p, max_tokens=args.max_tokens)
    outputs = llm.generate(prompts, sampling_params)
    df["response"] = [o.outputs[0].text.strip() for o in outputs]

    result_df = evaluator(df, args)

    if args.output is None:
        safe_model = args.model.replace("/", "_")
        safe_data = args.dataset.replace("/", "_")
        args.output = f"results/{safe_data}-{safe_model}.csv"
    
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    result_df.to_csv(args.output, index=False, encoding='utf-8-sig')
    print(f"âœ…ğŸ’¾ {len(result_df)}ê°œ í–‰ì˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ âœ {args.output}")


if __name__ == "__main__":
    main()