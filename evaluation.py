#!/usr/bin/env python
import argparse
import os
import torch
from vllm import LLM, SamplingParams
from datasets import load_dataset
from tqdm import tqdm
from dataset_configs import get_config, DATASET_CONFIGS

# LiteLLMì„ ìœ„í•œ API í‚¤ ì„¤ì •ì´ í•„ìš”í•˜ë©´ ì—¬ê¸°ì—ì„œ
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

def parse_args():
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì¸ìë“¤ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    p = argparse.ArgumentParser(description="Run a vLLM model on a dataset and save its responses.")
    p.add_argument("--model", required=True, help="Hugging Face model ID or local path")
    p.add_argument("--dataset", required=True,
                   help=f"Dataset name. Supported: {list(DATASET_CONFIGS.keys())}")
    p.add_argument("--dataset_hub_id", default='HAERAE-HUB/KoSimpleEval', help="Hugging Face Hub ID for the dataset collection")
    p.add_argument("--split", default="test", help="Dataset split (default: test)")
    p.add_argument("--max_tokens", type=int, default=2048, help="Maximum tokens to generate")
    p.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature (0.0 for deterministic output)")
    p.add_argument("--top_p", type=float, default=1.0, help="Top-p / nucleus sampling (1.0 to disable)")
    p.add_argument("--output", default=None, help="Output CSV path (auto-generated if omitted)")
    return p.parse_args()

def main() -> None:
    """ë©”ì¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    args = parse_args()

    # 1. ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    try:
        config = get_config(args.dataset)
        prompt_maker = config['prompt_maker']
        evaluator = config['evaluator']
    except ValueError as e:
        print(f"ì˜¤ë¥˜: {e}")
        return
    
    print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘: {args.model}")
    # Use CPU for testing
    from transformers import AutoTokenizer, AutoModelForCausalLM
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(args.model, device_map="cpu", trust_remote_code=True)
    
    # Create a simple wrapper class to mimic vLLM's interface
    class SimpleGenerationOutput:
        def __init__(self, text):
            self.text = text
            
    class SimpleOutput:
        def __init__(self, text):
            self.outputs = [SimpleGenerationOutput(text)]
            
    class SimpleLLM:
        def __init__(self, model, tokenizer):
            self.model = model
            self.tokenizer = tokenizer
            self.max_length = model.config.max_position_embeddings
            
        def get_tokenizer(self):
            return self.tokenizer
            
        def generate(self, prompts, sampling_params):
            outputs = []
            for prompt in prompts:
                try:
                    # Truncate input if it's too long
                    inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.max_length - sampling_params.max_tokens)
                    
                    output_ids = self.model.generate(
                        inputs.input_ids, 
                        max_new_tokens=sampling_params.max_tokens,
                        do_sample=sampling_params.temperature > 0,
                        temperature=max(sampling_params.temperature, 1e-5),  # Avoid division by zero
                        top_p=sampling_params.top_p
                    )
                    
                    output_text = self.tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                    outputs.append(SimpleOutput(output_text))
                except Exception as e:
                    print(f"Error generating response: {e}")
                    outputs.append(SimpleOutput("Error generating response"))
            return outputs
    
    llm = SimpleLLM(model, tokenizer)

    print(f"ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {args.dataset_hub_id} - {args.dataset}")
    df = load_dataset(args.dataset_hub_id, args.dataset, split=args.split).to_pandas()
    
    # For testing purposes, use only a small subset of the data
    if len(df) > 5:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë°ì´í„°ì…‹ì„ 5ê°œ ìƒ˜í”Œë¡œ ì œí•œí•©ë‹ˆë‹¤ (ì›ë˜ í¬ê¸°: {len(df)})")
        df = df.head(5)

    print("âœï¸  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    tqdm.pandas(desc="í”„ë¡¬í”„íŠ¸ ìƒì„±")
    prompts = df.progress_apply(lambda row: prompt_maker(row, tokenizer), axis=1).tolist()

    print(f"ğŸ§  ëª¨ë¸ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤ (ì´ {len(prompts)}ê°œ)...")
    sampling_params = SamplingParams(temperature=args.temperature, top_p=args.top_p, max_tokens=args.max_tokens)
    outputs = llm.generate(prompts, sampling_params)
    df["response"] = [o.outputs[0].text.strip() for o in outputs]

    result_df = evaluator(df, args)

    if args.output is None:
        safe_model = args.model.replace("/", "_")
        safe_data = args.dataset.replace("/", "_")
        args.output = f"results/{safe_data}-{safe_model}.csv"
    
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    result_df.to_csv(args.output, index=False, encoding='utf-8-sig')
    print(f"âœ…ğŸ’¾ {len(result_df)}ê°œ í–‰ì˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ âœ {args.output}")


if __name__ == "__main__":
    main()