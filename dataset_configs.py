import re
import json
import pandas as pd
from functools import partial
from litellm import batch_completion
from tqdm import tqdm

# ===================================================================
# í‰ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===================================================================

def _parse_math_answer(response: str) -> str:
    """ìˆ˜í•™ ë¬¸ì œ ì‘ë‹µì—ì„œ \\boxed{} ì•ˆì˜ ìµœì¢… ë‹µì•ˆì„ ì¶”ì¶œ"""
    match = re.search(r"\\boxed\{(.*?)\}", response)
    if match:
        return match.group(1).strip()
    # \\boxed{} í˜•ì‹ì´ ì—†ì„ ê²½ìš° -> ê± ì‹¤íŒ¨ì¸ê°€? ì¼ë‹¨ ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìˆ«ìž(ì •ìˆ˜/ì‹¤ìˆ˜)ë¥¼ ì¶”ì¶œ.
    numbers = re.findall(r"[-+]?\d*\.?\d+", response) # ì´ë ‡ê²Œ í•´ë„ ìž˜ ì•ˆë‚˜ì˜¬ ê²ƒ ê°™ê¸´ í•¨. í•„ìš”ì—†ìœ¼ë©´ ì œê±°
    return numbers[-1] if numbers else None

def _normalize_mqa_choice(choice: str) -> str:
    """ê°ê´€ì‹ ë¬¸ì œì˜ ì„ íƒì§€ë¥¼ ì •ê·œí™”(ì˜ˆ: 'A)', 'B.' -> 'A', 'B')"""
    if not isinstance(choice, str):
        return None
    return choice.strip().upper().strip(".)")

def _parse_answer_from_prompt(response: str, task="kobalt") -> str:
    if task == "kobalt":
        match = re.search(r"ì •ë‹µì€\s*([A-J])\s*ìž…ë‹ˆë‹¤", response)
        if match:
            return match.group(1).strip()
    return None  # TODO: add parsing for unknown

# ===================================================================
# 1. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ (Prompt Makers)
# ===================================================================
# ê° í•¨ìˆ˜ëŠ” ë°ì´í„°í”„ë ˆìž„ì˜ í–‰(row)ê³¼ í† í¬ë‚˜ì´ì €(tokenizer)ë¥¼ ë°›ì•„ ëª¨ë¸ì— ìž…ë ¥ë  ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¬¸ìžì—´ì„ ë°˜í™˜í•˜ëŠ” ì—­í• 

def _create_prompt_for_mqa(row, tokenizer):
    """ê°ê´€ì‹ ë¬¸ì œ(KMMLU, kmmlu-pro)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸"""
    # TODO: ë°ì´í„°ì…‹ì˜ 'choices' ì»¬ëŸ¼ í¬ë§·ì— ë§žì¶° í”„ë¡¬í”„íŠ¸ ì„¸ë¶€ ì¡°ì • í•„ìš” -> KMMLU, KMMLU-Proë§Œì´ë©´ í¬ê²Œ ìƒê´€ ì—†ì„ì§€ë„
    # ì˜ˆ: A. {row.choices[0]}\nB. {row.choices[1]}...
    query = (
        f"ë‹¤ìŒì€ '{row['category']}' ë¶„ì•¼ì˜ ê°ê´€ì‹ ë¬¸ì œìž…ë‹ˆë‹¤. "
        f"ê°€ìž¥ ì ì ˆí•œ ì„ íƒì§€ë¥¼ í•˜ë‚˜ë§Œ ê³¨ë¼ ê·¸ ì•ŒíŒŒë²³ì„ ë‹µí•´ì£¼ì„¸ìš”.\n\n"
        f"ë¬¸ì œ: {row['question']}\n\n"
        "ì„ íƒì§€:\n"
        f"{row['choices']}\n\n" # 'choices' ì»¬ëŸ¼ì´ ì„ íƒì§€ë¥¼ í¬í•¨í•œ ë¬¸ìžì—´ì´ë¼ê³  ê°€ì •
        "ì •ë‹µ:"
    )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_kobalt(row, tokenizer):
    """Kobalt (https://arxiv.org/pdf/2505.16125) Figure 2."""
    query = (
        f"ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•´ì„œ ì¶©ë¶„ížˆ ìƒê°í•˜ê³  ì¶”ë¡ í•˜ì—¬, 10ê°œì˜ ë³´ê¸°(A, B, C, D, E, F, G, H, I, J) ì¤‘ ì •ë‹µì„ ê³ ë¥´ì„¸ìš”.\n"
        f"{row['question']}\n\n"
        f"ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì—„ê²©ížˆ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤: \"ì •ë‹µì€ [ì •ë‹µ ë³´ê¸°]ìž…ë‹ˆë‹¤.\"ë¡œ ëë‚˜ì•¼í•˜ê³ , [ì •ë‹µ ë³´ê¸°]ëŠ” A, B, C, D, E, F, G, H, I, J ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n"
        f"ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´, í•œë²ˆ ì²œì²œížˆ ìƒê°í•´ë´…ì‹œë‹¤."
    )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_math(row, tokenizer):
    """ìˆ˜í•™ ë¬¸ì œ(AIME, MCLM)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸"""
    query = (
        f"ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”. í’€ì´ ê³¼ì •ê³¼ í•¨ê»˜ ìµœì¢… ë‹µì„ '\\boxed{{ì •ë‹µ}}' í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.\n\n"
        f"ë¬¸ì œ: {row['question']}"
    )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_qa(row, tokenizer):
    """ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (GPQA, KoBALT ë“±)"""
    query = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {row['question']}"
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_arena(row, tokenizer):
    """Arena í˜•ì‹ ë°ì´í„°ì…‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸"""
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": row['prompts']}],
        tokenize=False, add_generation_prompt=True
    )

# ===================================================================
# 2. í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜ (Evaluators)
# ===================================================================
# ê° í•¨ìˆ˜ëŠ” ëª¨ë¸ ì‘ë‹µì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆìž„(df)ê³¼ ì‹¤í–‰ ì¸ìž(args)ë¥¼ ë°›ì•„ ì±„ì  ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆìž„ì„ ë°˜í™˜í•˜ëŠ” ì—­í• .

def _evaluate_mqa(df, args):
    """ê°ê´€ì‹ ë¬¸ì œ(KMMLU, kmmlu-pro) í‰ê°€"""
    print("ðŸ¤– ê°ê´€ì‹ ë¬¸ì œ(MQA) í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")

    df['pred_choice'] = df['response'].apply(_normalize_mqa_choice)
    df['gold_choice'] = df['gold'].apply(_normalize_mqa_choice)
    df['correct'] = (df['pred_choice'] == df['gold_choice'])

    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

def _evaluate_math(df, args):
    """ìˆ˜í•™ ë¬¸ì œ(AIME, MCLM) í‰ê°€"""
    print("ðŸ¤– ìˆ˜í•™ ë¬¸ì œ í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")

    df['pred_answer'] = df['response'].apply(_parse_math_answer)
    df['gold_answer'] = df['gold'].astype(str) # ì •ë‹µì„ ë¬¸ìžì—´ë¡œ í†µì¼

    df['correct'] = (df['pred_answer'] == df['gold_answer'])

    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

def _evaluate_hrm8k_ksm(df, args):
    """hrm8k-ksm í‰ê°€ë¥¼ ìœ„í•´ 'answer' ì»¬ëŸ¼ì„ 'gold'ë¡œ ë³€ê²½ í›„ math í‰ê°€ ì‹¤í–‰"""
    print("ðŸ¤– hrm8k-ksm ë°ì´í„°ì…‹ í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤ (ì»¬ëŸ¼ëª… ë³€ê²½ í›„)...")
    # 'answer' ì»¬ëŸ¼ì„ 'gold'ë¡œ ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤.
    if 'answer' in df.columns:
        df.rename(columns={'answer': 'gold'}, inplace=True)
    else:
        print("âš ï¸ 'answer' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ê¸°ì¡´ ìˆ˜í•™ í‰ê°€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    return _evaluate_math(df, args)

def _evaluate_kobalt(df, args):
    """Kobalt í‰ê°€"""
    print("ðŸ¤– Kobalt í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
    # TODO: debug
    df['pred_choice'] = df['response'].apply(partial(_parse_answer_from_prompt, task="kobalt"))
    df['gold_choice'] = df['gold'].apply(_normalize_mqa_choice)
    df['correct'] = (df['pred_choice'] == df['gold_choice'])
    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

# dataset_configs.py íŒŒì¼ì˜ í•¨ìˆ˜ ì˜ì—­ì— ì¶”ê°€

def _evaluate_gpqa_as_math(df, args):
    """gpqa-d í‰ê°€ë¥¼ ìœ„í•´ 'Correct Answer'ë¥¼ 'gold'ë¡œ ë³€ê²½ í›„ math í‰ê°€ ì‹¤í–‰"""
    print("ðŸ¤– gpqa-diamond ë°ì´í„°ì…‹ì„ ìˆ˜í•™ ë¬¸ì œë¡œ í‰ê°€í•©ë‹ˆë‹¤...")

    # 'Correct Answer' ì»¬ëŸ¼ì˜ ì´ë¦„ì„ 'gold'ë¡œ ë³€ê²½
    if 'Correct Answer' in df.columns:
        # df.renameì€ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆìž„ì„ ë°˜í™˜í•˜ë¯€ë¡œ ë‹¤ì‹œ í• ë‹¹í•´ì•¼ í•¨
        df = df.rename(columns={'Correct Answer': 'gold'})
    else:
        print("âš ï¸ 'Correct Answer' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df

    # ê¸°ì¡´ ìˆ˜í•™ í‰ê°€ í•¨ìˆ˜(_evaluate_math)ë¥¼ í˜¸ì¶œ
    return _evaluate_math(df, args)

def _evaluate_arena(df, args):
    """Arena ë°ì´í„°ì…‹ í‰ê°€ (ì•ˆì •ì ì¸ JSON ëª¨ë“œ ì‚¬ìš©)"""
    print("ðŸ¤– Arena í‰ê°€ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤ (Judge ëª¨ë¸: gpt-4-turbo, ë°©ì‹: JSON Mode)...")

    # JSON ì¶œë ¥ í˜•ì‹ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì‹œí•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = (
        "You are a helpful and precise assistant for checking the quality of AI responses. "
        "Compare Response A and Response B based on the user's instruction. "
        "Your decision must be one of the following categories: "
        "B>>A, B>A, B=A, A>B, A>>B. "
        "Please provide your response in a JSON object with two keys: "
        "'verdict' (the category of your decision) and 'reasoning' (a brief explanation)."
    )

    qrys = []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="Arena í‰ê°€ ì§ˆë¬¸ ìƒì„±"):
        # ArenaHard ì™¸ ë‹¤ë¥¸ ë°ì´í„°ì…‹ í•„ìš” í•˜ë©´ ì°¸ì¡° ì‘ë‹µ(ref) ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”
        ref_response = row.get('ref', 'N/A')
        query_content = f"### Instruction:\n{row['prompts']}\n\n### Response A:\n{ref_response}\n\n### Response B:\n{row['response']}"
        qrys.append([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query_content}
        ])

    responses = batch_completion(
        model='gpt-4-turbo', # ëª¨ë¸ ëª… ë°”ê¿€ê±°ë©´ ë°”ê¾¸ê¸°
        messages=qrys,
        response_format={"type": "json_object"}
    )

    # ê¸°ì¡´ ì½”ë“œì˜ ì ìˆ˜í‘œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    score_map = {
        'A>>B': 0, 'A>B': 2, 'A=B': 3, 'A<B': 4, 'A<<B': 5, # A<B, A<<BëŠ” B>A, B>>Aì™€ ë™ì¼
        'B>>A': 5, 'B>A': 4, 'B=A': 3, 'B<A': 2, 'B<<A': 1
    }

    verdicts = []
    scores = []
    for res in tqdm(responses, desc="ì‹¬íŒ ì‘ë‹µ íŒŒì‹± ë° ì±„ì "):
        try:
            judge_json = json.loads(res.choices[0].message.content)
            verdict = judge_json.get('verdict')
            verdicts.append(verdict)
            scores.append(score_map.get(verdict))
        except (json.JSONDecodeError, AttributeError):
            # íŒŒì‹± ì‹¤íŒ¨
            verdicts.append("PARSE_ERROR")
            scores.append(None)

    df['judge_verdict'] = verdicts
    df['score'] = scores

    avg_score = df['score'].mean(skipna=True)
    error_count = df['score'].isna().sum()

    print(f"âœ… Arena í‰ê°€ ì™„ë£Œ! í‰ê·  ì ìˆ˜: {avg_score:.3f} (ì˜¤ë¥˜: {error_count}ê°œ)")
    return df

def _evaluate_placeholder(df, args):
    """êµ¬í˜„ì´ í•„ìš”í•œ ë°ì´í„°ì…‹ì„ ìœ„í•œ ìž„ì‹œ í‰ê°€ í•¨ìˆ˜""" # ì•„ì§ ì•ˆëœì• ë“¤ì— ê½‚ì•„ë‘ê¸° ìš©
    print(f"âš ï¸  '{args.dataset}' ë°ì´í„°ì…‹ì˜ í‰ê°€ ë¡œì§ì´ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²°ê³¼ íŒŒì¼ë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
    # TODO: í•´ë‹¹ ë°ì´í„°ì…‹ì— ë§žëŠ” í‰ê°€ ë¡œì§ì„ êµ¬í˜„ í•„ìš”
    df['correct'] = 'N/A'
    return df

# ===================================================================
# 3. ë°ì´í„°ì…‹ ì„¤ì • ì¢…í•© (ë©”ì¸ ì»¨íŠ¸ë¡¤)
# ===================================================================
DATASET_CONFIGS = {
    'KMMLU-Redux': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'MCLM': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'ArenaHard': {'prompt_maker': _create_prompt_for_arena, 'evaluator': _evaluate_arena},
    'kmmlu-pro': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'aime2025': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'aime2024': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'click': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'kobalt': {'prompt_maker': _create_prompt_for_kobalt, 'evaluator': _evaluate_kobalt},
    'KSM': {'prompt_maker': _create_prompt_for_qa, 'evaluator': _evaluate_hrm8k_ksm},
    'gpqa-diamond': {'prompt_maker': _create_prompt_for_qa, 'evaluator': _evaluate_gpqa_as_math},
}

def get_config(dataset_name):
    """ë°ì´í„°ì…‹ ì´ë¦„ì— ë§žëŠ” ì„¤ì •(í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜, í‰ê°€ í•¨ìˆ˜)ì„ ë°˜í™˜ë°ìŠ¤."""
    config = DATASET_CONFIGS.get(dataset_name)
    if config is None:
        raise ValueError(f"'{dataset_name}'ì— ëŒ€í•œ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `dataset_configs.py`ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    return config
