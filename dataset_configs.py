import re
import json
import pandas as pd
from functools import partial
from litellm import batch_completion
from tqdm import tqdm

# math_verify ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•˜ê³ , ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê°€ìš©ì„± í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
try:
    from math_verify import parse, verify
    MATH_VERIFY_AVAILABLE = True
    print("âœ… 'math_verify' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸ 'math_verify' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install math-verify ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    MATH_VERIFY_AVAILABLE = False


# ===================================================================
# í‰ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===================================================================

def _parse_math_answer(response: str) -> str:
    """ìˆ˜í•™ ë¬¸ì œ ì‘ë‹µì—ì„œ \\boxed{} ì•ˆì˜ ìµœì¢… ë‹µì•ˆì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r"\\boxed\{(.*?)\}", response)
    if match:
        return match.group(1).strip()
    # \\boxed{} í˜•ì‹ì´ ì—†ì„ ê²½ìš°, ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìˆ«ì(ì •ìˆ˜/ì‹¤ìˆ˜)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    numbers = re.findall(r"[-+]?\d*\.?\d+", response)
    return numbers[-1] if numbers else None

def _normalize_mqa_choice_advanced(response: str) -> str:
    """
    ëª¨ë¸ì˜ ë‹¤ì–‘í•œ ê°ê´€ì‹ ë‹µë³€ í˜•ì‹(<think> íƒœê·¸, ìˆ«ì ë‹µë³€ ë“±)ì„
    ì •ê·œí™”í•˜ì—¬ ìµœì¢… ì„ íƒì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not isinstance(response, str):
        return None

    # <think> íƒœê·¸ê°€ ìˆì„ ê²½ìš°, ê·¸ ì´í›„ì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if '</think>' in response:
        response = response.split('</think>', 1)[1]

    # "ì •ë‹µì€ [X]ì…ë‹ˆë‹¤" í˜•ì‹ì—ì„œ ì„ íƒì§€ë¥¼ ìš°ì„  ì¶”ì¶œí•©ë‹ˆë‹¤.
    match = re.search(r"ì •ë‹µì€\s*([A-Ja-j1-5])\s*(?:ì…ë‹ˆë‹¤|ì´ë‹¤)", response)
    if match:
        return match.group(1).upper().strip(".)")

    # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì–¸ê¸‰ëœ ì„ íƒì§€(A-E, 1-5)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    choices = re.findall(r'[A-Ea-e1-5]', response)
    if choices:
        last_choice = choices[-1]
        # ìˆ«ì í˜•ì‹ì˜ ë‹µë³€ì„ ì•ŒíŒŒë²³ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì˜ˆ: '1' -> 'A').
        if last_choice.isdigit():
            return chr(ord('A') + int(last_choice) - 1)
        return last_choice.upper()

    return response.strip().upper().strip(".)")

def _parse_answer_from_prompt(response: str, task="kobalt") -> str:
    """Kobalt ë°ì´í„°ì…‹ì˜ íŠ¹ì • ë‹µë³€ í˜•ì‹ì—ì„œ ì •ë‹µì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if task == "kobalt":
        match = re.search(r"ì •ë‹µì€\s*([A-J])\s*ì…ë‹ˆë‹¤", response)
        if match:
            return match.group(1).strip()
    return None

# ===================================================================
# 1. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ (Prompt Makers)
# ===================================================================

def _create_prompt_for_mqa(row, tokenizer):
    """ê°ê´€ì‹ ë¬¸ì œ(KMMLU, kmmlu-pro)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # choices ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ë¥¼ ëª¨ë‘ ì²˜ë¦¬
    if 'choices' in row and row['choices']:
        query = (
            f"ë‹¤ìŒì€ '{row['category']}' ë¶„ì•¼ì˜ ê°ê´€ì‹ ë¬¸ì œì…ë‹ˆë‹¤. "
            f"ê°€ì¥ ì ì ˆí•œ ì„ íƒì§€ë¥¼ í•˜ë‚˜ë§Œ ê³¨ë¼ ê·¸ ì•ŒíŒŒë²³ì„ ë‹µí•´ì£¼ì„¸ìš”.\n\n"
            f"ë¬¸ì œ: {row['question']}\n\n"
            "ì„ íƒì§€:\n"
            f"{row['choices']}\n\n"
            "ì •ë‹µ:"
        )
    else:
        # choices ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° (KoSimpleEval ë“±), questionì— ì„ íƒì§€ê°€ í¬í•¨ë˜ì–´ ìˆìŒ
        query = (
            f"ë‹¤ìŒì€ '{row['category']}' ë¶„ì•¼ì˜ ê°ê´€ì‹ ë¬¸ì œì…ë‹ˆë‹¤. "
            f"ê°€ì¥ ì ì ˆí•œ ì„ íƒì§€ë¥¼ í•˜ë‚˜ë§Œ ê³¨ë¼ ê·¸ ë²ˆí˜¸ë‚˜ ì•ŒíŒŒë²³ì„ ë‹µí•´ì£¼ì„¸ìš”.\n\n"
            f"{row['question']}\n\n"
            "ì •ë‹µ:"
        )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_kobalt(row, tokenizer):
    """Kobalt ë°ì´í„°ì…‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    query = (
        f"ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•´ì„œ ì¶©ë¶„íˆ ìƒê°í•˜ê³  ì¶”ë¡ í•˜ì—¬, 10ê°œì˜ ë³´ê¸°(A, B, C, D, E, F, G, H, I, J) ì¤‘ ì •ë‹µì„ ê³ ë¥´ì„¸ìš”.\n"
        f"{row['question']}\n\n"
        f"ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ì—„ê²©íˆ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤: \"ì •ë‹µì€ [ì •ë‹µ ë³´ê¸°]ì…ë‹ˆë‹¤.\"ë¡œ ëë‚˜ì•¼í•˜ê³ , [ì •ë‹µ ë³´ê¸°]ëŠ” A, B, C, D, E, F, G, H, I, J ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n"
        f"ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´, í•œë²ˆ ì²œì²œíˆ ìƒê°í•´ë´…ì‹œë‹¤."
    )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_math(row, tokenizer):
    """ìˆ˜í•™ ë¬¸ì œ(AIME, MCLM)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    question_text = row.get('question', row.get('problem', ''))
    query = (
        f"ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”. í’€ì´ ê³¼ì •ê³¼ í•¨ê»˜ ìµœì¢… ë‹µì„ '\\boxed{{ì •ë‹µ}}' í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.\n\n"
        f"ë¬¸ì œ: {question_text}"
    )
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_qa(row, tokenizer):
    """ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    query = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {row['question']}"
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

def _create_prompt_for_arena(row, tokenizer):
    """Arena í˜•ì‹ ë°ì´í„°ì…‹ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    query = row['prompts']
    return tokenizer.apply_chat_template(
        [{"role": "user", "content": query}],
        tokenize=False, add_generation_prompt=True
    )

# ===================================================================
# 2. í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜ (Evaluators)
# ===================================================================

def _evaluate_mqa(df, args):
    """ê°ê´€ì‹ ë¬¸ì œ(KMMLU ë“±)ë¥¼ ì •êµí•œ íŒŒì‹± ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤."""
    print("ğŸ¤– ê°ê´€ì‹ ë¬¸ì œ(MQA) í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Advanced Parsing)...")

    df['pred_choice'] = df['response'].apply(_normalize_mqa_choice_advanced)
    df['gold_choice'] = df['gold'].astype(str).str.upper().str.strip(".)")
    df['correct'] = (df['pred_choice'] == df['gold_choice'])

    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

def _evaluate_math(df, args):
    """ìˆ˜í•™ ë¬¸ì œë¥¼ `math_verify` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤."""
    print("ğŸ¤– ìˆ˜í•™ ë¬¸ì œ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Math-Verify Logic)...")

    if MATH_VERIFY_AVAILABLE:
        correct_output = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Math-Verify ê²€ì¦"):
            is_correct = False
            try:
                gold_answer = str(row.get('gold', row.get('answer')))
                parsed_gold = parse(gold_answer)
                parsed_pred = parse(row['response'])
                is_correct = verify(parsed_gold, parsed_pred)
            except Exception:
                is_correct = False
            correct_output.append(is_correct)
        df['correct'] = correct_output
    else:
        # math_verifyê°€ ì—†ì„ ê²½ìš°, ê¸°ì¡´ì˜ ë‹¨ìˆœ ì¶”ì¶œ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
        print("... 'math_verify'ê°€ ì—†ì–´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì±„ì í•©ë‹ˆë‹¤.")
        df['pred_answer'] = df['response'].apply(_parse_math_answer)
        if 'gold' in df.columns:
            df['gold_answer'] = df['gold'].astype(str)
        elif 'answer' in df.columns:
            df['gold_answer'] = df['answer'].astype(str)
        else:
            df['gold_answer'] = 'N/A'
        df['correct'] = (df['pred_answer'] == df['gold_answer'])

    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

def _evaluate_hrm8k_ksm(df, args):
    """hrm8k-ksm ë°ì´í„°ì…‹ì„ í‰ê°€í•©ë‹ˆë‹¤. 'answer' ì»¬ëŸ¼ì„ 'gold'ë¡œ ë³€ê²½ í›„ ìˆ˜í•™ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("ğŸ¤– hrm8k-ksm ë°ì´í„°ì…‹ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    if 'answer' in df.columns:
        df.rename(columns={'answer': 'gold'}, inplace=True)
    else:
        print("âš ï¸ 'answer' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df
    return _evaluate_math(df, args)

def _evaluate_kobalt(df, args):
    """Kobalt ë°ì´í„°ì…‹ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    print("ğŸ¤– Kobalt í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    df['pred_choice'] = df['response'].apply(partial(_parse_answer_from_prompt, task="kobalt"))
    df['gold_choice'] = df['gold'].astype(str).str.upper().str.strip(".)")
    df['correct'] = (df['pred_choice'] == df['gold_choice'])
    accuracy = df['correct'].mean()
    print(f"âœ… í‰ê°€ ì™„ë£Œ! ì „ì²´ ì •í™•ë„: {accuracy:.2%}")
    return df

def _evaluate_gpqa_as_math(df, args):
    """gpqa-diamond ë°ì´í„°ì…‹ì„ ìˆ˜í•™ ë¬¸ì œì²˜ëŸ¼ í‰ê°€í•©ë‹ˆë‹¤."""
    print("ğŸ¤– gpqa-diamond ë°ì´í„°ì…‹ì„ ìˆ˜í•™ ë¬¸ì œë¡œ í‰ê°€í•©ë‹ˆë‹¤...")
    if 'Correct Answer' in df.columns:
        df = df.rename(columns={'Correct Answer': 'gold'})
    else:
        print("âš ï¸ 'Correct Answer' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return df
    return _evaluate_math(df, args)

def _evaluate_arena(df, args):
    """Arena ë°ì´í„°ì…‹ì„ ì™¸ë¶€ LLMì„ ì´ìš©í•´ í‰ê°€í•©ë‹ˆë‹¤."""
    print("ğŸ¤– Arena í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Judge ëª¨ë¸: gpt-4-turbo, ë°©ì‹: JSON Mode)...")
    system_prompt = (
        "You are a helpful and precise assistant for checking the quality of AI responses. "
        "Compare Response A and Response B based on the user's instruction. "
        "Your decision must be one of the following categories: "
        "B>>A, B>A, B=A, A>B, A>>B. "
        "Please provide your response in a JSON object with two keys: "
        "'verdict' (the category of your decision) and 'reasoning' (a brief explanation)."
    )
    qrys = []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="Arena í‰ê°€ ì§ˆë¬¸ ìƒì„±"):
        ref_response = row.get('ref', 'N/A')
        query_content = f"### Instruction:\n{row['prompts']}\n\n### Response A:\n{ref_response}\n\n### Response B:\n{row['response']}"
        qrys.append([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query_content}
        ])

    responses = batch_completion(
        model='gpt-4.1-mini',
        messages=qrys,
        response_format={"type": "json_object"}
    )
    score_map = {
        'A>>B': 0, 'A>B': 2, 'A=B': 3, 'A<B': 4, 'A<<B': 5,
        'B>>A': 5, 'B>A': 4, 'B=A': 3, 'B<A': 2, 'B<<A': 1
    }
    verdicts, scores = [], []
    for res in tqdm(responses, desc="ì‹¬íŒ ì‘ë‹µ íŒŒì‹± ë° ì±„ì "):
        try:
            judge_json = json.loads(res.choices[0].message.content)
            verdict = judge_json.get('verdict')
            verdicts.append(verdict)
            scores.append(score_map.get(verdict))
        except (json.JSONDecodeError, AttributeError):
            verdicts.append("PARSE_ERROR")
            scores.append(None)

    df['judge_verdict'] = verdicts
    df['score'] = scores
    avg_score = df['score'].mean(skipna=True)
    error_count = df['score'].isna().sum()
    print(f"âœ… Arena í‰ê°€ ì™„ë£Œ! í‰ê·  ì ìˆ˜: {avg_score:.3f} (ì˜¤ë¥˜: {error_count}ê°œ)")
    return df

# ===================================================================
# 3. ë°ì´í„°ì…‹ ì„¤ì • ì¢…í•© (ë©”ì¸ ì»¨íŠ¸ë¡¤)
# ===================================================================
DATASET_CONFIGS = {
    # ê¸°ì¡´ ì„¤ì •ë“¤
    'KMMLU_Redux': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'ClinicalQA':  {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'KMMLU-Pro': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa}, 
    'KMMLU-HARD': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa}, 
    'KorMedLawQA': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa}, 
    'MCLM': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'ArenaHard': {'prompt_maker': _create_prompt_for_arena, 'evaluator': _evaluate_arena},
    'aime_2025': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'aime_2024': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'default': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'train': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'click': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'kobalt': {'prompt_maker': _create_prompt_for_kobalt, 'evaluator': _evaluate_kobalt},
    'KSM': {'prompt_maker': _create_prompt_for_qa, 'evaluator': _evaluate_hrm8k_ksm},
    'gpqa-diamond': {'prompt_maker': _create_prompt_for_qa, 'evaluator': _evaluate_gpqa_as_math},
    
    # KoSimpleEval ë°ì´í„°ì…‹ configë“¤ ì¶”ê°€
    'HRB1_0': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'KMMLU': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'CLIcK': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'GPQA': {'prompt_maker': _create_prompt_for_mqa, 'evaluator': _evaluate_mqa},
    'KoBALT-700': {'prompt_maker': _create_prompt_for_kobalt, 'evaluator': _evaluate_kobalt},
    'AIME2024': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
    'AIME2025': {'prompt_maker': _create_prompt_for_math, 'evaluator': _evaluate_math},
}

def get_config(dataset_name):
    """ë°ì´í„°ì…‹ ì´ë¦„ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ì™€ í‰ê°€ í•¨ìˆ˜ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config = DATASET_CONFIGS.get(dataset_name)
    if config is None:
        raise ValueError(f"'{dataset_name}'ì— ëŒ€í•œ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `dataset_configs.py`ì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    return config